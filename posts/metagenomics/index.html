<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">

    <meta name="author" content="Ravin Poudel">
    <meta name="description" content="Welcome to metagenomics tutorial !!! These tutorials walk you through the process of analyzing shotgun metagenomics data. We will cover:
 Assembly Based Metagenomics Read Based Metagenomics  Assembly Based Metagenomics Assembling reads into contigs and mapping reads to the assembled contigs are the primary steps in assembly-based metagenomics. This tutorial walks you through the various steps involved in the process of genome assembly and mapping.
Step 1: Quality Check Prior to genome assembly, reads are cleaned using a custom ARS-RQC pipeline, which utilizes a suite of functions from BBtools to remove host genome contamination and trim the standard Illumina barcodes.">
    <meta name="keywords" content="bioinformatics,genomics,personal">

    
      <script src="https://twemoji.maxcdn.com/v/latest/twemoji.min.js" crossorigin="anonymous"></script>
    

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Metagenomics tutorial"/>
<meta name="twitter:description" content="Welcome to metagenomics tutorial !!! These tutorials walk you through the process of analyzing shotgun metagenomics data. We will cover:
 Assembly Based Metagenomics Read Based Metagenomics  Assembly Based Metagenomics Assembling reads into contigs and mapping reads to the assembled contigs are the primary steps in assembly-based metagenomics. This tutorial walks you through the various steps involved in the process of genome assembly and mapping.
Step 1: Quality Check Prior to genome assembly, reads are cleaned using a custom ARS-RQC pipeline, which utilizes a suite of functions from BBtools to remove host genome contamination and trim the standard Illumina barcodes."/>

    <meta property="og:title" content="Metagenomics tutorial" />
<meta property="og:description" content="Welcome to metagenomics tutorial !!! These tutorials walk you through the process of analyzing shotgun metagenomics data. We will cover:
 Assembly Based Metagenomics Read Based Metagenomics  Assembly Based Metagenomics Assembling reads into contigs and mapping reads to the assembled contigs are the primary steps in assembly-based metagenomics. This tutorial walks you through the various steps involved in the process of genome assembly and mapping.
Step 1: Quality Check Prior to genome assembly, reads are cleaned using a custom ARS-RQC pipeline, which utilizes a suite of functions from BBtools to remove host genome contamination and trim the standard Illumina barcodes." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ravinpoudel.github.io/posts/metagenomics/" />
<meta property="article:published_time" content="2019-06-16T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-06-16T00:00:00+00:00" />


    
      <base href="https://ravinpoudel.github.io/posts/metagenomics/">
    
    <title>
  Metagenomics tutorial · Ravin Poudel
</title>

    
      <link rel="canonical" href="https://ravinpoudel.github.io/posts/metagenomics/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.13.0/css/all.css" integrity="sha384-Bfad6CLCknfcloXFOyFnlgtENryhrpZCe29RTifKEixXQZ38WheV+i/6YWSzkz3V" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="/css/coder.min.3219ef62ae52679b7a9c19043171c3cd9f523628c2a65f3ef247ee18836bc90b.css" integrity="sha256-MhnvYq5SZ5t6nBkEMXHDzZ9SNijCpl8&#43;8kfuGINryQs=" crossorigin="anonymous" media="screen" />
    

    

    
      
        
        
        <link rel="stylesheet" href="/css/coder-dark.min.e78e80fc3a585a4d1c8fc7f58623b6ff852411e38431a9cd1792877ecaa160f6.css" integrity="sha256-546A/DpYWk0cj8f1hiO2/4UkEeOEManNF5KHfsqhYPY=" crossorigin="anonymous" media="screen" />
      
    

    

    

    <link rel="icon" type="image/png" href="https://ravinpoudel.github.io/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://ravinpoudel.github.io/images/favicon-16x16.png" sizes="16x16">

    <meta name="generator" content="Hugo 0.72.0" />
  </head>

  
  
    
  
  <body class="colorscheme-auto"
        onload=" twemoji.parse(document.body); "
  >
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      Ravin Poudel
    </a>
    
    <input type="checkbox" id="menu-toggle" />
    <label class="menu-button float-right" for="menu-toggle"><i class="fas fa-bars"></i></label>
    <ul class="navigation-list">
      
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://ravinpoudel.github.io/about/">About</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://ravinpoudel.github.io/posts/">Blog</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://ravinpoudel.github.io/publications/">Publications</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://ravinpoudel.github.io/projects/">Projects</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://ravinpoudel.github.io/contact/">Contact</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://ravinpoudel.github.io/images/RavinPoudel.pdf">Resume</a>
          </li>
        
      
      
    </ul>
    
  </section>
</nav>


      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Metagenomics tutorial</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fas fa-calendar"></i>
              <time datetime='2019-06-16T00:00:00Z'>
                June 16, 2019
              </time>
            </span>
            <span class="reading-time">
              <i class="fas fa-clock"></i>
              9-minute read
            </span>
          </div>
          
          <div class="tags">
  <i class="fas fa-tag"></i>
    <a href="/tags/microbial-ecology/">Microbial Ecology</a>
      <span class="separator">•</span>
    <a href="/tags/metagenomics/">Metagenomics</a></div>

        </div>
      </header>

      <div>
        
        <h1 id="welcome-to-metagenomics-tutorial-">Welcome to metagenomics tutorial !!!</h1>
<p>These tutorials walk you through the process of analyzing shotgun metagenomics data. We will cover:</p>
<ul>
<li><strong>Assembly Based Metagenomics</strong></li>
<li><strong>Read Based Metagenomics</strong></li>
</ul>
<p><img src="/images/Metagenomics.png" alt="alt text"></p>
<h2 id="assembly-based-metagenomics">Assembly Based Metagenomics</h2>
<p>Assembling reads into contigs and mapping reads to the assembled contigs are the primary steps in assembly-based metagenomics. This tutorial walks you through the various steps involved in the process of genome assembly and mapping.</p>
<p><img src="/images/Assembly-Based-Metagenomics.png" alt="alt text"></p>
<h1 id="step-1-quality-check">Step 1: Quality Check</h1>
<p>Prior to genome assembly, reads are cleaned using a custom <a href="https://github.com/USDA-ARS-GBRU/ARS-RQC">ARS-RQC pipeline</a>, which utilizes a suite of functions from <a href="https://jgi.doe.gov/data-and-tools/bbtools/">BBtools</a> to remove host genome contamination and trim the standard Illumina barcodes. JSON files obtained from ARS-RQC can be exported as csv file and read in R to create a summary file.</p>
<p><em>1-1. Create a conda environment</em></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">conda create -n Ametagenomics python<span style="color:#f92672">=</span>3.6
source activate Ametagenomics
conda install pandas
conda install numpy
conda install -c conda-forge mkl_random 
conda install -c anaconda cython y
conda install -c bioconda bbmap
</code></pre></div><p><em>1-2. Get ars-rqc from the repository and install the editable version. This allows to auto-update the changes made in the repository – so that the used program will be the most recent one.</em></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">git clone https://github.com/USDA-ARS-GBRU/ARS-RQC.git
cd ARS-RQC
pip install --editable .
</code></pre></div><p><em>1-3. Soft link the input files from collaborator’s raw-data directory.</em></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir data
cd data
ln -vs /project/**/**/*.fastq.gz .
</code></pre></div><p><em>1-4. Run <code>01_arsqc_interleave.sh</code> to create interleaved fastq.</em></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e">#!/bin/bash
</span><span style="color:#75715e"></span><span style="color:#75715e">#SBATCH --job-name=rqc_interleave_Ametagenomics</span>
<span style="color:#75715e">#SBATCH --output=rqc_Ametagenomics_%A_%a.out</span>
<span style="color:#75715e">#SBATCH --error=rqc_Ametagenomics_%A_%a.err</span>
<span style="color:#75715e">#SBATCH --time=01:00:00</span>
<span style="color:#75715e">#SBATCH --array=1-156</span>
<span style="color:#75715e">#SBATCH -p short</span>
<span style="color:#75715e">#SBATCH -N 1</span>
<span style="color:#75715e">#SBATCH -n 10</span>

module load miniconda
source activate Ametagenomics
module load pigz
module load bbtools/gcc/64/36.86

RUN<span style="color:#f92672">=</span><span style="color:#e6db74">${</span>SLURM_ARRAY_TASK_ID<span style="color:#e6db74">}</span>
echo <span style="color:#e6db74">&#34;My Slurm RUN_ID: &#39;</span><span style="color:#e6db74">${</span>RUN<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;&#34;</span>
echo <span style="color:#e6db74">&#34;My TMPDIR IS: &#34;</span> $TMPDIR

infile1<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>ls data/*_R1.fastq.gz | sed -n <span style="color:#e6db74">${</span>RUN<span style="color:#e6db74">}</span>p<span style="color:#66d9ef">)</span>
echo <span style="color:#e6db74">&#34;</span>$infile1<span style="color:#e6db74">&#34;</span>

infile2<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>echo $infile1 | sed <span style="color:#e6db74">&#39;s/R1/R2/&#39;</span><span style="color:#66d9ef">)</span>
echo <span style="color:#e6db74">&#34;</span>$infile2<span style="color:#e6db74">&#34;</span>

outbase<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>basename $infile1 _R1.fastq.gz<span style="color:#66d9ef">)</span>
outfile<span style="color:#f92672">=</span>data/<span style="color:#e6db74">${</span>outbase<span style="color:#e6db74">}</span>_interleave.fastq.gz
echo <span style="color:#e6db74">&#34;</span>$outfile<span style="color:#e6db74">&#34;</span>

reformat.sh in<span style="color:#f92672">=</span>$infile1 in2<span style="color:#f92672">=</span>$infile2 out<span style="color:#f92672">=</span>$outfile
</code></pre></div><p><em>1-5. Run  <code>02_arsqc_filer.sh</code> to run qc on fastq.</em></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e">#!/bin/bash
</span><span style="color:#75715e"></span><span style="color:#75715e">#SBATCH --job-name=rqc_filter_Ametagenomics</span>
<span style="color:#75715e">#SBATCH --output=rqc_Ametagenomics_%A_%a.out</span>
<span style="color:#75715e">#SBATCH --error=rqc_Ametagenomics_%A_%a.err</span>
<span style="color:#75715e">#SBATCH --time=01:00:00</span>
<span style="color:#75715e">#SBATCH --array=1-156</span>
<span style="color:#75715e">#SBATCH -p short</span>
<span style="color:#75715e">#SBATCH -N 1</span>
<span style="color:#75715e">#SBATCH -n 10</span>

module load miniconda
source activate Ametagenomics
module load pigz


RUN<span style="color:#f92672">=</span><span style="color:#e6db74">${</span>SLURM_ARRAY_TASK_ID<span style="color:#e6db74">}</span>
echo <span style="color:#e6db74">&#34;My Slurm RUN_ID: &#39;</span><span style="color:#e6db74">${</span>RUN<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;&#34;</span>
echo <span style="color:#e6db74">&#34;My TMPDIR IS: &#34;</span> $TMPDIR

rootdir<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;/project/&#34;</span>
echo <span style="color:#e6db74">&#34;</span>$rootdir<span style="color:#e6db74">&#34;</span>


itl_file<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>ls data/*_interleave.fastq.gz | sed -n <span style="color:#e6db74">${</span>RUN<span style="color:#e6db74">}</span>p<span style="color:#66d9ef">)</span>
echo <span style="color:#e6db74">&#34;</span>$itl_file<span style="color:#e6db74">&#34;</span>
time rqcfilter.py --fastq $itl_file --output <span style="color:#e6db74">&#34;</span>$rootdir<span style="color:#e6db74">&#34;</span>rqc_data -p
</code></pre></div><p><em>1-6. Run <code>03_parse_json.py</code> – create a summary data-frame using json files obtained from qc. This file is then analyzed in R to create qc-report using R markdown.</em></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> json
<span style="color:#f92672">import</span> pandas <span style="color:#f92672">as</span> pd
<span style="color:#f92672">import</span> os

wdir <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>getcwd()

list <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">for</span> lfile <span style="color:#f92672">in</span> os<span style="color:#f92672">.</span>listdir(<span style="color:#e6db74">&#34;rqc_data&#34;</span>):
    file <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(wdir, <span style="color:#e6db74">&#34;rqc_data&#34;</span>, lfile)
    <span style="color:#66d9ef">if</span> file<span style="color:#f92672">.</span>endswith(<span style="color:#e6db74">&#39;.json&#39;</span>):
        bfile <span style="color:#f92672">=</span> file
        <span style="color:#66d9ef">with</span> open(file) <span style="color:#66d9ef">as</span> js:
            js_dict <span style="color:#f92672">=</span> json<span style="color:#f92672">.</span>load(js)
            tr <span style="color:#f92672">=</span> js_dict[<span style="color:#e6db74">&#39;scaffoldStats1.txt&#39;</span>][<span style="color:#e6db74">&#39;desc&#39;</span>][<span style="color:#e6db74">&#39;TotalReads&#39;</span>]
            tb <span style="color:#f92672">=</span> js_dict[<span style="color:#e6db74">&#39;scaffoldStats1.txt&#39;</span>][<span style="color:#e6db74">&#39;desc&#39;</span>][<span style="color:#e6db74">&#39;TotalBases&#39;</span>]
            contam <span style="color:#f92672">=</span> js_dict[<span style="color:#e6db74">&#39;scaffoldStats1.txt&#39;</span>][<span style="color:#e6db74">&#39;desc&#39;</span>][<span style="color:#e6db74">&#39;ReadsMatched&#39;</span>]
            pctcontam <span style="color:#f92672">=</span> js_dict[<span style="color:#e6db74">&#39;scaffoldStats1.txt&#39;</span>][<span style="color:#e6db74">&#39;desc&#39;</span>][<span style="color:#e6db74">&#39;PctReadsMatched&#39;</span>]
            list<span style="color:#f92672">.</span>append((os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>basename(bfile), tr, tb, contam, pctcontam))

cols <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;SampleID&#39;</span>, <span style="color:#e6db74">&#39;TotalReads&#39;</span>, <span style="color:#e6db74">&#39;TotalBases&#39;</span>, <span style="color:#e6db74">&#39;Contaminants&#39;</span>, <span style="color:#e6db74">&#34;Percent_Contaminants&#34;</span>]
result <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(list, columns<span style="color:#f92672">=</span>cols)
result<span style="color:#f92672">.</span>to_csv(<span style="color:#e6db74">&#34;rqc_data/parse_json.csv&#34;</span>)

</code></pre></div><h1 id="step-2-de-novo-assembly">Step 2: De Novo Assembly</h1>
<p>At this point, we have clean reads that are ready for assembling to contigs. Given we have a large number of fastq files (here 156), lets first create a list of file names and separate each file name with a comma, then pass to megahit.</p>
<p><em>2-1. Create a list of files, open in text editor, then find carry-over and replace with comma, then delete comma at the EOF. This information will be pass as input files for genome assembly in megahit.</em></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">find /project/rqc_data/reads/** -type f &gt; file_list_withpath.txt

</code></pre></div><p><em>2-2. Run genome assembly <code>04_megahit.sh</code></em></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e">#!/bin/bash
</span><span style="color:#75715e"></span><span style="color:#75715e">#SBATCH --job-name=megahit_assembly</span>
<span style="color:#75715e">#SBATCH --output=megahit_assembly_%A_%a.out</span>
<span style="color:#75715e">#SBATCH --error=megahit_assembly_%A_%a.err</span>
<span style="color:#75715e">#SBATCH --time=96:00:00</span>
<span style="color:#75715e">#SBATCH -p mem</span>
<span style="color:#75715e">#SBATCH -N 1</span>
<span style="color:#75715e">#SBATCH -n 120</span>

export PATH<span style="color:#f92672">=</span>$PATH:/home/ravin.poudel/bbmap
export PATH<span style="color:#f92672">=</span>$PATH:/home/ravin.poudel/megahit/1.1.1
module load miniconda
<span style="color:#75715e">#source activate Ametagenomics</span>
module load pigz
module load megahit

echo <span style="color:#e6db74">&#34;My TMPDIR IS: &#34;</span> $TMPDIR

time megahit --k-min <span style="color:#ae81ff">27</span> --k-max <span style="color:#ae81ff">127</span> --k-step <span style="color:#ae81ff">10</span> -m 0.98 -t <span style="color:#ae81ff">120</span> --out-dir megahit_output --kmin-1pass --min-contig-len <span style="color:#ae81ff">300</span> --tmp-dir $TMPDIR --12 /project/rqc_data/reads/A1_run1_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/A1_run2_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/A1_run2_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/A1_run2_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/A1_run2_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/A1_run3_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/A1_run3_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/A1_run3_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/A1_run3_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/A1_run4_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/A1_run4_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/A1_run4_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/A1_run4_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/A2_run1_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/A2_run2_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/A2_run2_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/A2_run2_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/A2_run2_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/A2_run3_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/A2_run3_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/A2_run3_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/A2_run3_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/A2_run4_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/A2_run4_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/A2_run4_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/A2_run4_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/A3_run1_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/A3_run2_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/A3_run2_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/A3_run2_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/A3_run2_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/A3_run3_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/A3_run3_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/A3_run3_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/A3_run3_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/A3_run4_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/A3_run4_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/A3_run4_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/A3_run4_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/C1W_run1_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/C1W_run2_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/C1W_run2_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/C1W_run2_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/C1W_run2_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/C1W_run3_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/C1W_run3_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/C1W_run3_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/C1W_run3_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/C1W_run4_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/C1W_run4_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/C1W_run4_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/C1W_run4_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/C2W_run1_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/C2W_run2_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/C2W_run2_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/C2W_run2_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/C2W_run2_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/C2W_run3_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/C2W_run3_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/C2W_run3_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/C2W_run3_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/C2W_run4_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/C2W_run4_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/C2W_run4_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/C2W_run4_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/C3W_run1_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/C3W_run2_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/C3W_run2_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/C3W_run2_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/C3W_run2_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/C3W_run3_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/C3W_run3_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/C3W_run3_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/C3W_run3_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/C3W_run4_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/C3W_run4_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/C3W_run4_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/C3W_run4_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/M1_run1_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/M1_run2_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/M1_run2_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/M1_run2_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/M1_run2_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/M1_run3_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/M1_run3_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/M1_run3_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/M1_run3_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/M1_run4_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/M1_run4_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/M1_run4_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/M1_run4_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/M2_run1_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/M2_run2_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/M2_run2_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/M2_run2_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/M2_run2_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/M2_run3_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/M2_run3_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/M2_run3_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/M2_run3_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/M2_run4_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/M2_run4_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/M2_run4_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/M2_run4_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/M3_run1_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/M3_run2_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/M3_run2_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/M3_run2_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/M3_run2_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/M3_run3_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/M3_run3_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/M3_run3_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/M3_run3_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/M3_run4_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/M3_run4_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/M3_run4_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/M3_run4_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/O1S_run1_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/O1S_run2_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/O1S_run2_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/O1S_run2_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/O1S_run2_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/O1S_run3_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/O1S_run3_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/O1S_run3_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/O1S_run3_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/O1S_run4_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/O1S_run4_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/O1S_run4_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/O1S_run4_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/O2S_run1_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/O2S_run2_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/O2S_run2_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/O2S_run2_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/O2S_run2_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/O2S_run3_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/O2S_run3_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/O2S_run3_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/O2S_run3_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/O2S_run4_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/O2S_run4_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/O2S_run4_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/O2S_run4_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/O3S_run1_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/O3S_run2_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/O3S_run2_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/O3S_run2_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/O3S_run2_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/O3S_run3_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/O3S_run3_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/O3S_run3_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/O3S_run3_lane4_interleave.rqc.fq.gz,/project/rqc_data/reads/O3S_run4_lane1_interleave.rqc.fq.gz,/project/rqc_data/reads/O3S_run4_lane2_interleave.rqc.fq.gz,/project/rqc_data/reads/O3S_run4_lane3_interleave.rqc.fq.gz,/project/rqc_data/reads/O3S_run4_lane4_interleave.rqc.fq.gz

</code></pre></div><h1 id="step-3-mapping">Step 3: Mapping</h1>
<p>Output obtained after the assembly is also a fasta file, assembled contig. This assembled contigs will be used for mapping the reads. Prior we mapped the reads, we can concatenate/merge reads from technical replicates. In our case for each experimental unit, we have 13 fastq files.</p>
<p><em>3-1. Run <code>05_merge_interleaved_fastq.sh</code> to merge fastq by technical replicates.</em></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e">#!/bin/bas
</span><span style="color:#75715e"></span><span style="color:#75715e">#SBATCH --job-name=merge</span>
<span style="color:#75715e">#SBATCH --output=merge_%A_%a.out</span>
<span style="color:#75715e">#SBATCH --error=merge_%A_%a.err</span>
<span style="color:#75715e">#SBATCH --time=01:00:00</span>
<span style="color:#75715e">#SBATCH -p short</span>
<span style="color:#75715e">#SBATCH -N 1</span>
<span style="color:#75715e">#SBATCH -n 40</span>

cat reads/A1*.fq.gz &gt; merged_fastqByRelicates/A1_interleaved_merge.fq.gz
cat reads/A2*.fq.gz &gt; merged_fastqByRelicates/A2_interleaved_merge.fq.gz
cat reads/A3*.fq.gz &gt; merged_fastqByRelicates/A3_interleaved_merge.fq.gz
cat reads/M1*.fq.gz &gt; merged_fastqByRelicates/M1_interleaved_merge.fq.gz
cat reads/M2*.fq.gz &gt; merged_fastqByRelicates/M2_interleaved_merge.fq.gz
cat reads/M3*.fq.gz &gt; merged_fastqByRelicates/M3_interleaved_merge.fq.gz
cat reads/O1S*.fq.gz &gt; merged_fastqByRelicates/O1S_interleaved_merge.fq.gz
cat reads/O2S*.fq.gz &gt; merged_fastqByRelicates/O2S_interleaved_merge.fq.gz
cat reads/O3S*.fq.gz &gt; merged_fastqByRelicates/O3S_interleaved_merge.fq.gz
cat reads/C1W*.fq.gz &gt; merged_fastqByRelicates/C1W_interleaved_merge.fq.gz
cat reads/C2W*.fq.gz &gt; merged_fastqByRelicates/C2W_interleaved_merge.fq.gz
</code></pre></div><p><em>3-2. Also, prior to mapping reads, the genome needs to be indexed. This can be done using <code>bowtie2-build</code> function form bowtie2. Run <code>06_index_genome.sh</code>.</em></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e">#!/bin/bash
</span><span style="color:#75715e"></span>
<span style="color:#75715e">#SBATCH --job-name=index_contig</span>
<span style="color:#75715e">#SBATCH --output=index_contig_%A_%a.out</span>
<span style="color:#75715e">#SBATCH --error=index_contig_%A_%a.err</span>
<span style="color:#75715e">#SBATCH --time=04:00:00</span>
<span style="color:#75715e">#SBATCH -p short</span>
<span style="color:#75715e">#SBATCH -N 1</span>
<span style="color:#75715e">#SBATCH -n 40</span>

module load bowtie2/2.3.4
module load samtools


bowtie2-build megahit_output/final.contigs.fa megahit_output/contigs
</code></pre></div><p><em>3-3. Run <code>07_mapping.sh</code> to map reads to assembled genome using bowtie2.</em></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e">#!/bin/bash
</span><span style="color:#75715e"></span>
<span style="color:#75715e">#SBATCH --job-name=bowtie_map</span>
<span style="color:#75715e">#SBATCH --output=bowtie_map_%A_%a.out</span>
<span style="color:#75715e">#SBATCH --error=bowtie_map_%A_%a.err</span>
<span style="color:#75715e">#SBATCH --time=12:00:00</span>
<span style="color:#75715e">#SBATCH --array=1-12</span>
<span style="color:#75715e">#SBATCH -p short</span>
<span style="color:#75715e">#SBATCH -N 1</span>
<span style="color:#75715e">#SBATCH -n 40</span>

module load bowtie2/2.3.4
module load samtools

rootdir<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;/project/&#34;</span>
echo <span style="color:#e6db74">&#34;</span>$rootdir<span style="color:#e6db74">&#34;</span>

RUN<span style="color:#f92672">=</span><span style="color:#e6db74">${</span>SLURM_ARRAY_TASK_ID<span style="color:#e6db74">}</span>
echo <span style="color:#e6db74">&#34;My Slurm RUN_ID: &#39;</span><span style="color:#e6db74">${</span>RUN<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;&#34;</span>

iarray<span style="color:#f92672">=(</span><span style="color:#e6db74">&#34;</span>$rootdir<span style="color:#e6db74">&#34;</span>rqc_data/merged_fastqByRelicates/*_interleaved_merge.fq.gz<span style="color:#f92672">)</span>
echo <span style="color:#e6db74">&#34;</span>$iarray<span style="color:#e6db74">&#34;</span>

infile<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">${</span>iarray[$SLURM_ARRAY_TASK_ID - 1]<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
echo <span style="color:#e6db74">&#34;</span>$infile<span style="color:#e6db74">&#34;</span>

bname<span style="color:#f92672">=</span><span style="color:#e6db74">`</span>basename $infile _interleaved_merge.fq.gz<span style="color:#e6db74">`</span>
echo <span style="color:#e6db74">&#34;</span>$bname<span style="color:#e6db74">&#34;</span>

time bowtie2 -p <span style="color:#ae81ff">40</span> -x megahit_output/contigs --interleaved $infile -S <span style="color:#e6db74">&#34;</span>$rootdir<span style="color:#e6db74">&#34;</span>mapping/<span style="color:#e6db74">&#34;</span>$bname<span style="color:#e6db74">&#34;</span>.sam
time samtools sort -o <span style="color:#e6db74">&#34;</span>$rootdir<span style="color:#e6db74">&#34;</span>mapping/<span style="color:#e6db74">&#34;</span>$bname<span style="color:#e6db74">&#34;</span>.bam <span style="color:#e6db74">&#34;</span>$rootdir<span style="color:#e6db74">&#34;</span>mapping/<span style="color:#e6db74">&#34;</span>$bname<span style="color:#e6db74">&#34;</span>.sam

</code></pre></div><h1 id="step-4-calling-for-bins-or-metagenomes">Step 4: Calling for bins or metagenomes.</h1>
<p>At this point, we have obtained a large genomic fragment (genomic mess) representing microbial communities. We need to separate them into individual genomes/bins, for which we are using automated software called <code>MetaBAT</code>. MetaBAT accurately bins genomes using probabilistic distances of genome abundance and tetranucleotide frequency. More on <code>MetaBAT</code> is available at their <a href="https://bitbucket.org/berkeleylab/metabat">repo</a> and accompanied <a href="https://peerj.com/articles/1165/">paper</a>.</p>
<p><em>4-1. Run <code>08_metabat.sh</code></em></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e">#!/bin/bash
</span><span style="color:#75715e"></span><span style="color:#75715e">#SBATCH --job-name=metabat</span>
<span style="color:#75715e">#SBATCH --output=anvi-metabat_%A_%a.out</span>
<span style="color:#75715e">#SBATCH --error=anvi-metabat_%A_%a.err</span>
<span style="color:#75715e">#SBATCH --time=04:00:00</span>
<span style="color:#75715e">#SBATCH -p short</span>
<span style="color:#75715e">#SBATCH -N 1</span>
<span style="color:#75715e">#SBATCH -n 40</span>

module load miniconda
source activate metabat


runMetaBat.sh -m <span style="color:#ae81ff">5000</span> /project/megahit_output/final.contigs.fa /project/mapping/*.bam
</code></pre></div><h1 id="step-5-quality-check-of-metagenome-assembled-genomes-mags">Step 5: Quality check of Metagenome-assembled genomes (MAGs).</h1>
<p><em>5-1. Run <code>09_checkM.sh</code></em></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e">#!/bin/bash
</span><span style="color:#75715e"></span><span style="color:#75715e">#SBATCH --job-name=CHECKM</span>
<span style="color:#75715e">#SBATCH --output=anvi-metabat_%A_%a.out</span>
<span style="color:#75715e">#SBATCH --error=anvi-metabat_%A_%a.err</span>
<span style="color:#75715e">#SBATCH --time=04:00:00</span>
<span style="color:#75715e">#SBATCH -p short</span>
<span style="color:#75715e">#SBATCH -N 1</span>
<span style="color:#75715e">#SBATCH -n 40</span>

module load miniconda
module load checkm


checkm lineage_wf -t <span style="color:#ae81ff">40</span> -x fa final.contigs.fa.metabat-bins5000/ /projectq/checkM
</code></pre></div><p>Filtering bins is not an easy task, as criteria to define/filter bins is not hard and fast. We follow the guidelines for <a href="https://www.nature.com/articles/nbt.3893">MIMAG</a> such that the bins with completeness &gt; 95 % and contamination &lt; 5% were selected for downstream analyses.</p>
<p><img src="/images/MIMAG.png" alt="table"></p>
<h1 id="step-6-functional-annotation-of-mags">Step 6: Functional annotation of MAGs.</h1>
<p>To retrieve the relevant genomic features from the MAGs, we use <a href="https://github.com/tseemann/prokka">prokka</a></p>
<p><em>6-1. Run <code>10_porka_annotation.sh</code>, which usages prokka</em></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e">#!/bin/bash
</span><span style="color:#75715e"></span><span style="color:#75715e">#SBATCH --job-name=PORKA</span>
<span style="color:#75715e">#SBATCH --output=porka_bybins_%A_%a.out</span>
<span style="color:#75715e">#SBATCH --error=porka_bybins_%A_%a.err</span>
<span style="color:#75715e">#SBATCH --time=04:00:00</span>
<span style="color:#75715e">#SBATCH --array=1-19</span>
<span style="color:#75715e">#SBATCH -p short</span>
<span style="color:#75715e">#SBATCH -N 1</span>
<span style="color:#75715e">#SBATCH -n 40</span>


module load miniconda
source activate megan


RUN<span style="color:#f92672">=</span><span style="color:#e6db74">${</span>SLURM_ARRAY_TASK_ID<span style="color:#e6db74">}</span>
echo <span style="color:#e6db74">&#34;My Slurm RUN_ID: &#39;</span><span style="color:#e6db74">${</span>RUN<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;&#34;</span>
echo <span style="color:#e6db74">&#34;My TMPDIR IS: &#34;</span> $TMPDIR

infile<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>ls filter_bins_metabat_checkM/*.fa | sed -n <span style="color:#e6db74">${</span>RUN<span style="color:#e6db74">}</span>p<span style="color:#66d9ef">)</span>
echo <span style="color:#e6db74">&#34;</span>$infile<span style="color:#e6db74">&#34;</span>

outbase<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>basename $infile .fa<span style="color:#66d9ef">)</span>
outdirect<span style="color:#f92672">=</span>porka_bybins/<span style="color:#e6db74">${</span>outbase<span style="color:#e6db74">}</span>
echo <span style="color:#e6db74">&#34;</span>$outdirect<span style="color:#e6db74">&#34;</span>

prokka $infile --outdir $outdirect --prefix $outbase --addgenes --addmrna --centre X --compliant --genus --species --strain --kingdom --cpus <span style="color:#ae81ff">40</span>
</code></pre></div><p>For each MAGs, prokka provides the following files as output. These outputs can be explored for other downstream analyses such as metabolic pathways.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">PROKKA_11302018.daa <span style="color:#75715e">## useful to explore using megan</span>
PROKKA_11302018.err
PROKKA_11302018.faa
PROKKA_11302018.ffn
PROKKA_11302018.fna <span style="color:#75715e">## fasta file</span>
PROKKA_11302018.fsa
PROKKA_11302018.gbk  <span style="color:#75715e">## genebank file</span>
PROKKA_11302018.gff
PROKKA_11302018.log
PROKKA_11302018.sqn
PROKKA_11302018.tbl
PROKKA_11302018.tsv
PROKKA_11302018.txt
</code></pre></div><h1 id="humann2">HUMAnN2</h1>
<p>HUMAnN is a pipeline for efficiently and accurately profiling the presence/absence and abundance of microbial pathways and genes using shotgun metagenomics data. HUMAnN2 first search the reads against a database of known genes with known function. Unaligned/unmapped reads are translated and search against protein databases (UniRef90 / UniRef50). You can read more about HUMANn2 on the <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6235447/">paper</a> and <a href="https://bitbucket.org/biobakery/humann2/wiki/Home">repo</a></p>
<ul>
<li>HUMAnN2 used ChocoPhlAn database; proprietary database created by clustering the NCBI coding sequences.</li>
<li>Taxonomic profiling in HUMAnN2 in based on MetaPhlan2.</li>
<li>HUMAnN2 not only Profile the presence/absence (pathways coverage file), but also the abundance of microbial pathways in a community (pathways abundance file).
<img src="/images/humann2.jpg" alt="humann2"></li>
</ul>
<p><em><strong>Franzosa et. al. (2018)</strong></em></p>
<p>Running HUMAnN2 is quite minimal in term of scripting, but the code ran multiple steps as described in the following workflow. The main output from the HUMAnN2 includes three files:</p>
<h1 id="output-from-humann2">Output from HUMAnN2</h1>
<p><strong>-Gene families file</strong></p>
<ul>
<li>Abundance of each gene family in the community. These gene lists are used as a construct in MetaCyc reactions to build the MetaCyc pathways.</li>
</ul>
<p><strong>-Pathway abundance file</strong></p>
<ul>
<li>Abundance of MetaCyc pathways in each sample. HUMAnN2 uses MetaCyc pathway definitions and MinPath to identify a parsimonious set of pathways which explain observed reactions in the community.</li>
</ul>
<p><strong>-Pathway coverage file</strong></p>
<ul>
<li>How well the pathways in the &ldquo;Pathway abundance&rdquo; file are covered by the genes predicted in the Gene families file.</li>
</ul>
<h3 id="note">Note:</h3>
<p>Quantification of genes and pathways in HUMAn2 is units of RPKs (reads per kilobase). RPKs accounts for gene length but not sample sequencing depth. Options to normalize to relative abundance or copies per million (CPM) units are available. Both of these techniques follow the constant sum constraints, where in relative abundance samples are constrained to sum to 1, and samples are constrained to sum to 1 million in CPM.</p>
<p>In the temporary files, we can find the file &ldquo;&hellip;bug_list&rdquo; which provide the taxonomic profile.</p>
<h1 id="pipeline">Pipeline</h1>
<h1 id="step-1-run-humann2sh">Step 1: Run humann2.sh</h1>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e">#!/bin/bash
</span><span style="color:#75715e"></span><span style="color:#75715e">#SBATCH --job-name=humanN2</span>
<span style="color:#75715e">#SBATCH --output=humanN2_%A_%a.out</span>
<span style="color:#75715e">#SBATCH --error=humanN2_%A_%a.err</span>
<span style="color:#75715e">#SBATCH --time=24:00:00</span>
<span style="color:#75715e">#SBATCH --array=1-12</span>
<span style="color:#75715e">#SBATCH -p short</span>
<span style="color:#75715e">#SBATCH -N 1</span>
<span style="color:#75715e">#SBATCH -n 40</span>

module load miniconda
source activate humanN2
module load pigz


RUN<span style="color:#f92672">=</span><span style="color:#e6db74">${</span>SLURM_ARRAY_TASK_ID<span style="color:#e6db74">}</span>
echo <span style="color:#e6db74">&#34;My Slurm RUN_ID: &#39;</span><span style="color:#e6db74">${</span>RUN<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;&#34;</span>


infile1<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>ls rqc_data/merged_fastqByRelicates/*_interleaved_merge.fq.gz| sed -n <span style="color:#e6db74">${</span>RUN<span style="color:#e6db74">}</span>p<span style="color:#66d9ef">)</span>
echo <span style="color:#e6db74">&#34;</span>$infile1<span style="color:#e6db74">&#34;</span>


humann2 --threads <span style="color:#ae81ff">30</span> --input $infile1 --nucleotide-database rqc_data/chocophlan --protein-database rqc_data/uniref50/uniref --bowtie2 /home/ravin.poudel/.conda/envs/humanN2/bin/metaphlan_databases/ --search-mode uniref50 --output humann2_uniref50_merged_out/
</code></pre></div><p>For each sample(fastq), there are three output files:</p>
<ul>
<li>_pathcoverage.tsv</li>
<li>_pathabundance.tsv</li>
<li>_genefamilies.tsv</li>
</ul>
<h1 id="step-2-merge-output-into-a-single-file-and-normalize">Step 2: Merge output into a single file, and normalize.</h1>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e">#!/bin/bash
</span><span style="color:#75715e"></span><span style="color:#75715e">#SBATCH --job-name=mf</span>
<span style="color:#75715e">#SBATCH --output=mf_%A_%a.out</span>
<span style="color:#75715e">#SBATCH --error=mf_%A_%a.err</span>
<span style="color:#75715e">#SBATCH --time=00:30:00</span>
<span style="color:#75715e">#SBATCH -p short</span>
<span style="color:#75715e">#SBATCH -N 1</span>
<span style="color:#75715e">#SBATCH -n 10</span>


module load miniconda
source activate humanN2
module load pigz



<span style="color:#75715e"># Merge all genefiles into single file</span>
humann2_join_tables -i humann2_uniref50_merged_out/ -o all_genefamilies.tsv --file_name genefamilies 
humann2_join_tables -i humann2_uniref50_merged_out/ -o all_pathabundance.tsv --file_name pathabundance 
humann2_join_tables -i humann2_uniref50_merged_out/ -o all_pathcoverage.tsv --file_name pathcoverage 


<span style="color:#75715e"># # To facilitate comparisons between samples with different sequencing depths, it is beneficial to normalize gene</span>
<span style="color:#75715e"># family and pathway abundance RPK values to compositional units (copies per million or relative abundance)</span>


<span style="color:#75715e"># copies per million</span> 
humann2_renorm_table -i all_genefamilies.tsv -o all_genefamilies_cpm.tsv --units cpm
humann2_renorm_table -i all_pathabundance.tsv -o all_pathabundance_cpm.tsv --units cpm

<span style="color:#75715e"># relative abundance</span>
humann2_renorm_table -i all_genefamilies.tsv -o all_genefamilies_relab.tsv --units relab
humann2_renorm_table -i all_pathabundance.tsv -o all_pathabundance_relab.tsv --units relab
</code></pre></div><h1 id="step-3-taxonomic-profile">Step 3: Taxonomic profile</h1>
<p>Find the file name with *_bugs_list.tsv inside the temp output folders from step1, then cp them into a single folder, and merge to create a single file.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"> mkdir bug_list
 cp <span style="color:#66d9ef">$(</span>pwd<span style="color:#66d9ef">)</span>/**/*_bugs_list.tsv bug_list
 merge_metaphlan_tables.py *_bugs_list.tsv &gt; merged_bugs_list.tsv

</code></pre></div><h1 id="note-1">Note:</h1>
<p>Since sequence-based profiling is relative and does not provide absolute cellular abundance measures, clades are hierarchically summed. Each level will sum to 100%; that is, the sum of all kindom-level clades is 100%, the sum of all genus-level clades (including unclassified) is also 100%, and so forth.</p>

      </div>


      <footer>
        


        <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "https-ravinpoudel-github-io" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
        
        
      </footer>
    </article>

    
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
      
        ©
        
          2000 -
        
        2020
         Ravin Poudel 
      
      
         · 
        Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
      
      
    </section>
  </footer>

    </main>

    

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-99029062-1', 'auto');
	
	ga('send', 'pageview');
}
</script>


    

  </body>

</html>
